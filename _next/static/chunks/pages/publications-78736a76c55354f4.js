(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[548],{7202:function(e,a,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/publications",function(){return n(2500)}])},2294:function(e,a,n){"use strict";n.d(a,{Z:function(){return s}});var t=n(1527);n(959);var i=n(4357);function s(e){let{research:a,index:n}=e;return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)("div",{className:"flex flex-col md:flex-row py-5 px-0 lg:px-5 justify-center items-center content-center",children:[(0,t.jsx)("div",{className:"lg:mr-5 max-md:max-w-96 max-md:mx-5 md:basis-2/6 lg:basis-1/4 md:min-h-48 mx-auto rounded-2xl overflow-hidden border flex justify-center items-center content-center bg-white h-fit max-md:mb-5",children:(0,t.jsx)("img",{src:a.image,className:"object-cover",alt:a.title})}),(0,t.jsxs)("div",{className:"basis-full md:basis-4/6 lg:basis-3/4 pl-5 max-md:pr-5",children:[(0,t.jsx)("div",{className:"pb-2",children:(0,t.jsx)("a",{className:"text-lg font-bold ",href:a.link,children:a.title})}),a.authors.map((e,n)=>(0,t.jsxs)("div",{className:"inline",children:[void 0!==e.link?(0,t.jsx)("a",{href:e.link,className:"text-base hover:underline text-blue-500 dark:text-blue-300"+(e.me?" font-bold":" font-light"),children:e.name}):(0,t.jsx)("span",{className:"text-base text-gray-500 dark:text-gray-300 font-light",children:e.name}),n!==a.authors.length-1?(0,t.jsx)("span",{className:"text-base text-gray-500 dark:text-gray-300",children:", "}):(0,t.jsx)(t.Fragment,{})]},n)),(0,t.jsxs)("div",{className:"text-base text-black dark:text-white font-normal",children:[a.journal,", ",a.year]}),(0,t.jsx)("div",{className:"text-base text-gray-500 dark:text-gray-300 py-2 font-light max-sm:hidden ",children:a.description},""),Object.entries(a.others).map((e,a)=>{let[n,i]=e;return(0,t.jsxs)("div",{className:"inline pr-1",children:["[ ",(0,t.jsx)("a",{href:i,className:"hover:underline text-blue-500 dark:text-blue-300",children:n})," ]"]},a)})]})]}),n!==i.Z.researchList.length-1?(0,t.jsx)("div",{className:"mx-3 md:mx-0 lg:mx-3 border-gray-400 border box-border mt-5 mb-12 md:my-2"},n):(0,t.jsx)(t.Fragment,{})]})}},4357:function(e,a){"use strict";a.Z={interest:["I have a keen interest in exploring various aspects of machine learning, such as natural language processing and reinforcement learning. Representative papers are highlighted."],researchList:[{image:"/static/research/av-superb.png",title:"AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models",link:"https://av.superbbenchmark.org/",authors:[{name:"Yuan Tseng"},{name:"Layne Berry*"},{name:"Yi-Ting Chen*"},{name:"I-Hsiang Chiu*"},{name:"Hsuan-Hao Lin*"},{name:"Max Liu*"},{name:"Puyuan Peng*"},{name:"Yi-Jen Shih*"},{name:"Hung-Yu Wang*"},{name:"Haibin Wu*"},{name:"Po-Yao Huang"},{name:"Chun-Mao Lai",link:"https://www.mecoli.net/",me:!0},{name:"Shang-Wen Li"},{name:"David Harwath"},{name:"Yu Tsao"},{name:"Shinji Watanabe"},{name:"Abdelrahman Mohamed"},{name:"Chi-Luen Feng"},{name:"Hung-yi Lee",link:"https://speech.ee.ntu.edu.tw/~hylee/index.php"}],journal:"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",year:"2024",description:"We propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing.",others:{"Project Page":"https://av.superbbenchmark.org/",arXiv:"https://arxiv.org/abs/2309.10787",Code:"https://github.com/roger-tseng/av-superb"}},{image:"/static/research/dbc.png",title:"Diffusion Model-Augmented Behavioral Cloning",link:"https://nturobotlearninglab.github.io/dbc/",authors:[{name:"Hsiang-Chun Wang*"},{name:"Shang-Fu Chen*"},{name:"Ming-Hao Hsu",link:"https://qaz159qaz159.github.io/"},{name:"Chun-Mao Lai",link:"https://www.mecoli.net/",me:!0},{name:"Shao-Hua Sun",link:"https://shaohua0116.github.io/"}],journal:"Frontiers4LCD Workshop at International Conference on Machine Learning (ICML)",year:"2023",description:"We propose a novel imitation learning method combining with diffusion model. We show that our method can achieve better performance than previous imitation learning methods.",others:{"Project Page":"https://nturobotlearninglab.github.io/dbc/",arXiv:"https://arxiv.org/abs/2302.13335"}},{image:"/static/research/cuda-dst.png",title:"Controllable User Dialogue Act Augmentation for Dialogue State Tracking",link:"https://arxiv.org/abs/2207.12757",authors:[{name:"Chun-Mao Lai*",link:"https://www.mecoli.net/",me:!0},{name:"Ming-Hao Hsu*",link:"https://qaz159qaz159.github.io/"},{name:"Chao-Wei Huang",link:"https://scholar.google.com/citations?user=nmsPLncAAAAJ"},{name:"Yun-Nung Chen",link:"https://www.csie.ntu.edu.tw/~yvchen/"}],journal:"23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",year:"2022",description:"We propose a data augmentation method for DST, which improve the state-of-the-art performance on MultiWOZ 2.1.",others:{arXiv:"https://arxiv.org/abs/2207.12757",Code:"https://github.com/miulab/cuda-dst"}}]}},2500:function(e,a,n){"use strict";n.r(a);var t=n(1527),i=n(2294),s=n(4357);a.default=function(){return(0,t.jsx)(t.Fragment,{children:(0,t.jsxs)("div",{className:"max-w-screen mx-auto flex flex-col px-5 md:px-20",children:[(0,t.jsx)("h1",{className:"text-4xl sm:text-5xl md:text-5xl font-bold pt-10 pb-5 text-center",children:"Publications."}),(0,t.jsx)("div",{className:"text-container max-w-md mx-auto pt-2 border-t-2 border-gray-400",children:(0,t.jsx)("p",{className:"leading-normal text-xl pb-5 text-center text-gray-500 dark:text-gray-300",children:"Depth fuels expertise, breadth sparks innovation."})}),(0,t.jsx)("div",{className:"h-12"}),s.Z.researchList.map((e,a)=>(0,t.jsx)(i.Z,{research:e,index:a},a))]})})}}},function(e){e.O(0,[774,888,179],function(){return e(e.s=7202)}),_N_E=e.O()}]);